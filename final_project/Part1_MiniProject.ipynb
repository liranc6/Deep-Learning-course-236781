{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "$$\n",
    "\n",
    "# Part 1: Mini-Project\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this part you'll implement a small comparative-analysis project, heavily based on the materials from the tutorials and homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- You should implement the code which displays your results in this notebook, and add any additional code files for your implementation in the `project/` directory. You can import these files here, as we do for the homeworks.\n",
    "- Running this notebook should not perform any training - load your results from some output files and display them here. The notebook must be runnable from start to end without errors.\n",
    "- You must include a detailed write-up (in the notebook) of what you implemented and how.\n",
    "- Explain the structure of your code and how to run it to reproduce your results.\n",
    "- Explicitly state any external code you used, including built-in pytorch models and code from the course tutorials/homework.\n",
    "- Analyze your numerical results, explaining **why** you got these results (not just specifying the results).\n",
    "- Where relevant, place all results in a table or display them using a graph.\n",
    "- Before submitting, make sure all files which are required to run this notebook are included in the generated submission zip.\n",
    "- Try to keep the submission file size under 10MB. Do not include model checkpoint files, dataset files, or any other non-essentials files. Instead include your results as images/text files/pickles/etc, and load them for display in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Object detection on TACO dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TACO is a growing image dataset of waste in the wild. It contains images of litter taken under diverse environments: woods, roads and beaches.\n",
    "\n",
    "<center><img src=\"imgs/taco.png\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "you can read more about the dataset here: https://github.com/pedropro/TACO\n",
    "\n",
    "and can explore the data distribution and how to load it from here: https://github.com/pedropro/TACO/blob/master/demo.ipynb\n",
    "\n",
    "\n",
    "The stable version of the dataset that contain 1500 images and 4787 annotations exist in `datasets/TACO-master`\n",
    "You do not need to download the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Project goals:\n",
    "\n",
    "* You need to perform Object Detection task, over 7 of the dataset.\n",
    "* The annotation for object detection can be downloaded from here: https://github.com/wimlds-trojmiasto/detect-waste/tree/main/annotations.\n",
    "* The data and annotation format is like the COCOAPI: https://github.com/cocodataset/cocoapi (you can find a notebook of how to perform evalutation using it here: https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb)\n",
    "(you need to install it..)\n",
    "* if you need a beginner guild for OD in COCOAPI, you can read and watch this link: https://www.neuralception.com/cocodatasetapi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### What do i need to do?\n",
    "\n",
    "* **Everything is in the game!** as long as your model does not require more then 8 GB of memory and you follow the Guidelines above.\n",
    "\n",
    "\n",
    "### What does it mean?\n",
    "* you can use data augmentation, rather take what's implemented in the directory or use external libraries such as https://albumentations.ai/ (notice that when you create your own augmentations you need to change the annotation as well)\n",
    "* you can use more data if you find it useful (for examples, reviwew https://github.com/AgaMiko/waste-datasets-review)\n",
    "\n",
    "\n",
    "### What model can i use?\n",
    "* Whatever you want!\n",
    "you can review good models for the coco-OD task as a referance:\n",
    "SOTA: https://paperswithcode.com/sota/object-detection-on-coco\n",
    "Real-Time: https://paperswithcode.com/sota/real-time-object-detection-on-coco\n",
    "Or you can use older models like YOLO-V3 or Faster-RCNN\n",
    "* As long as you have a reason (complexity, speed, preformence), you are golden.\n",
    "\n",
    "### Tips for a good grade:\n",
    "* start as simple as possible. dealing with APIs are not the easiest for the first time and i predict that this would be your main issue. only when you have a running model that learn, you can add learning tricks.\n",
    "* use the visualization of a notebook, as we did over the course, check that your input actually fitting the model, the output is the desired size and so on.\n",
    "* It is recommanded to change the images to a fixed size, like shown in here :https://github.com/pedropro/TACO/blob/master/detector/inspect_data.ipynb\n",
    "* Please adress the architecture and your loss function/s in this notebook. if you decided to add some loss component like the Focal loss for instance, try to show the results before and after using it.\n",
    "* Plot your losses in this notebook, any evaluation metric can be shown as a function of time and possibe to analize per class.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model selection\n",
    "We experimented with 2 models, a faster-rcnn with a resnet50 back-bone as well as yolo-v8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# faster-rcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It appears it mainly learned to detect bottles and it did so quite well with a confidence of 0.65 and 0.79 for the last 2 images respectively.\n",
    "Other classes were more of a problem.\n",
    "for example in the following pictures it detected various pieces of objects but not always in their entirety.\n",
    "<img src=\"project/soap.png\" />\n",
    "<img src=\"project/spray2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "These problems are also due to the RCNN being a quite limited model and not very good at generalizing, especially with so little data.\n",
    "becuase\n",
    "of this it overfit and just learned one of the classes to a much higher degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# YOLOV8\n",
    "\n",
    "## Model research\n",
    "\n",
    "We decided to do a web-research for other OD models.\n",
    "YOLO in general and YOLOV8 in particular was a leading recommended model.\n",
    "We saw that \"YOLO makes less than half the number of background errors\n",
    "as compared to Faster R-CNN\" (https://www.irjmets.com/uploadedfiles/paper//issue_9_september_2022/30226/final/fin_irjmets1664212182.pdf).\n",
    "We also discovered that YoloV8 was very performant on COCO dataset. Because of the similarities between TACO and COCO, we decided to try it on our dataset.\n",
    "Furthermore, after using Yolov3 in HW2, we felt encouraged to use what we learned.\n",
    "\n",
    "## Model architecture\n",
    "\n",
    "YOLOv8 does not yet have a published paper, but there is a visualization made by a GitHub user named RangeKing. This visual is recognized and approved by the Ultralytics team.\n",
    "\n",
    "TODO: add image\n",
    "\n",
    "## Loss and metrics\n",
    "\n",
    "The mean average precision (mAP) is a common metric used to measure the performance of models doing object detection tasks.\n",
    "It measures the accuracy of object detection by considering both precision and recall.\n",
    "\n",
    "For object detection, we use the concept of Intersection over Union (IoU), as we saw in the YOLOv8 tutorial. IoU measures the overlap between 2 boundaries. We use it to estimate how well predicted and the ground truth bounding box overlap. Average Precision (AP) is the average over multiple IoU.\n",
    "mAP is calculated as the average of the Average Precision (AP) values for each object class, which are obtained by plotting the precision-recall curve for the class and computing the area under the curve. Higher mAP values indicate better object detection accuracy, with 100% being a perfect score.\n",
    "\n",
    "# Code\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Install pyTorch\n",
    "Read how to do that on: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LIRAN\\Desktop\\technion\\winter_23\\deep\\project\\Using_YOLOv8\\ultralytics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ultralytics'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/LIRAN/Desktop/technion/winter_23/deep/project/Using_YOLOv8/ultralytics\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (1.24.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (4.7.0.72)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (9.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (2.28.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (1.10.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (1.10.1)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (0.11.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (4.65.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (1.5.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (0.12.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (5.9.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: sentry_sdk in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from ultralytics==8.0.61) (1.18.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics==8.0.61) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics==8.0.61) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics==8.0.61) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics==8.0.61) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics==8.0.61) (5.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics==8.0.61) (23.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics==8.0.61) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics==8.0.61) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from pandas>=1.1.4->ultralytics==8.0.61) (2022.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.0.61) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.0.61) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.0.61) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.0.61) (1.26.14)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from torch>=1.7.0->ultralytics==8.0.61) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from tqdm>=4.64.0->ultralytics==8.0.61) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.2.2->ultralytics==8.0.61) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\liran\\miniconda3\\envs\\cs236781-hw\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->ultralytics==8.0.61) (3.11.0)\n",
      "Installing collected packages: ultralytics\n",
      "  Running setup.py develop for ultralytics\n",
      "Successfully installed ultralytics-8.0.61\n",
      "C:\\Users\\LIRAN\\Desktop\\technion\\winter_23\\deep\\project\\Using_YOLOv8\n"
     ]
    }
   ],
   "source": [
    "# Yoy can install YOLOv8 using pip or by cloning the repo\n",
    "# I will clone the repo\n",
    "\n",
    "if not os.path.exists('./ultralytics'): #first run\n",
    "    !git clone https://github.com/ultralytics/ultralytics\n",
    "    %cd ultralytics\n",
    "    !pip install -e .\n",
    "    %cd ..\n",
    "    #The flag \"-e\" stands for \"editable\" and allows the installed package to be modified in-place, meaning that changes made to the source code will be immediately reflected in the installed package.\n",
    "    #The dot \".\" at the end of the command indicates that the package to be installed is in the current working directory.\n",
    "else:\n",
    "    print(\"YOLOv8 already cloned to directory ultralytics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The requirements may be incompatible to your PC oe server. Make sure you finished last stage before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Create Dataset\n",
    "\n",
    "YOLOv8 models must be trained on labelled data in order to learn classes of objects in that data. There are two options for creating a dataset before starting the training:\n",
    "1. Use Roboflow to manage the dataset in YOLO format\n",
    "2. manually prepare the dataset\n",
    "\n",
    "We will create the dataset manually and use TACO dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.1 Download TACO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LIRAN\\Desktop\\technion\\winter_23\\deep\\project\\Using_YOLOv8\\TACO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'TACO'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LIRAN\\Desktop\\technion\\winter_23\\deep\\project\\Using_YOLOv8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: download.py [-h] [--dataset_path DATASET_PATH]\n",
      "download.py: error: unrecognized arguments: #can take several minutes\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./TACO'): #first run\n",
    "    !git clone https://github.com/pedropro/TACO\n",
    "    %cd TACO\n",
    "    #can take several minutes\n",
    "    #!pip install -r requirements.txt\n",
    "    !python download.py\n",
    "    %cd ..\n",
    "else:\n",
    "    print(\"TACO already cloned to directory TACO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "anns_file = './TACO/data/annotations.json'\n",
    "with open(project_path + anns_file, 'r') as j_f:\n",
    "    annotations = json.load(j_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.2 Organize folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join(project_path, 'tmp_taco_dataset')\n",
    "save_image_path = os.path.join(save_path, 'images')\n",
    "save_label_path = os.path.join(save_path, 'labels')\n",
    "if not os.path.exists(save_label_path):\n",
    "    os.makedirs(save_label_path)\n",
    "if not os.path.exists(save_image_path):\n",
    "    os.makedirs(save_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Prepare labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1 convert taco label to detect-waste labels\n",
    "The label conversion is based on the paper \"Deep learning-based waste detection in natural and urban environments\"\n",
    "https://www.sciencedirect.com/science/article/pii/S0956053X21006474?dgcid=coauthor#fn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define classes\n",
    "classes = { 'glass': ['Glass bottle','Broken glass','Glass jar'],\n",
    "            'metals_and_plastics': ['Aluminium foil', \"Clear plastic bottle\",\"Other plastic bottle\",\n",
    "                     \"Plastic bottle cap\",\"Metal bottle cap\",\"Aerosol\",\"Drink can\",\n",
    "                     \"Food can\",\"Drink carton\",\"Disposable plastic cup\",\"Other plastic cup\",\n",
    "                     \"Plastic lid\",\"Metal lid\",\"Single-use carrier bag\",\"Polypropylene bag\",\n",
    "                     \"Plastic Film\",\"Six pack rings\",\"Spread tub\",\"Tupperware\",\n",
    "                     \"Disposable food container\",\"Other plastic container\",\n",
    "                     \"Plastic glooves\",\"Plastic utensils\",\"Pop tab\",\"Scrap metal\",\n",
    "                     \"Plastic straw\",\"Other plastic\", \"Plastic film\", \"Food Can\"],\n",
    "            'non_recyclable': [\"Aluminium blister pack\",\"Carded blister pack\",\n",
    "                \"Meal carton\",\"Pizza box\",\"Cigarette\",\"Paper cup\",\n",
    "                \"Meal carton\",\"Foam cup\",\"Glass cup\",\"Wrapping paper\",\n",
    "                \"Magazine paper\",\"Garbage bag\",\"Plastified paper bag\",\n",
    "                \"Crisp packet\",\"Other plastic wrapper\",\"Foam food container\",\n",
    "                \"Rope\",\"Shoe\",\"Squeezable tube\",\"Paper straw\",\"Styrofoam piece\", \"Rope & strings\", \"Tissues\"],\n",
    "            'other': [\"Battery\"],\n",
    "            'paper': [\"Corrugated carton\",\"Egg carton\",\"Toilet tube\",\"Other carton\", \"Normal paper\", \"Paper bag\"],\n",
    "            'bio': [\"Food waste\"],\n",
    "            'unknown': [\"Unlabeled litter\"]\n",
    "            }\n",
    "classes_ids = {k: i for i, k in enumerate(classes.keys())}\n",
    "classes_inv = {k: v for v in classes.keys() for k in classes[v]}\n",
    "\n",
    "cat_names = {cat['id']: cat['name'] for cat in annotations['categories']}\n",
    "\n",
    "def map_categories_to_class(cat_id: int):\n",
    "    cat_name = cat_names[cat_id]\n",
    "    if cat_name in classes_inv.keys():\n",
    "        cls = classes_inv[cat_name]\n",
    "        cls_id = classes_ids[cls]\n",
    "        return cls_id\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 Create labels *.txt files for every image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LIRAN\\Desktop\\technion\\winter_23\\deep\\project\\Using_YOLOv8\\TACO\\data\n"
     ]
    }
   ],
   "source": [
    "%cd ./TACO/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop over the images in the annotations file\n",
    "for img in annotations['images']:\n",
    "    # Get the image width and height\n",
    "    img_width = img['width']\n",
    "    img_height = img['height']\n",
    "\n",
    "    # Get the image filename\n",
    "    img_file = img['file_name']\n",
    "\n",
    "    image_path = os.path.join(save_image_path, img_file.replace('/', '_'))\n",
    "    label_path = os.path.join(save_label_path, img_file.replace('/', '_').split('.')[0]+'.txt')\n",
    "\n",
    "    shutil.copy(img_file, image_path)\n",
    "\n",
    "    # Open the corresponding YOLO annotation file\n",
    "    yolo_file = open(label_path, 'w')\n",
    "    # Loop over the annotations for this image\n",
    "    for ann in annotations['annotations']:\n",
    "        # If the annotation corresponds to this image\n",
    "        if ann['image_id'] == img['id']:\n",
    "            # Get the object class and bounding box coordinates\n",
    "            # round the result up to 6 numbers afet decimal point\n",
    "            class_id = map_categories_to_class(ann['category_id'])\n",
    "            if class_id is None:\n",
    "                print('error')\n",
    "                break\n",
    "            bbox = ann['bbox']\n",
    "            x_center = round((bbox[0] + bbox[2] / 2) / img_width, 6)\n",
    "            y_center = round((bbox[1] + bbox[3] / 2) / img_height, 6)\n",
    "            width = round(bbox[2] / img_width, 6)\n",
    "            height = round(bbox[3] / img_height, 6)\n",
    "            # Write the annotation in YOLO format to the annotation file\n",
    "            yolo_file.write('{} {} {} {} {}\\n'.format(class_id, x_center, y_center, width, height))\n",
    "            # If there are target type objects, copy the image to the specified directory\n",
    "    # Close the YOLO annotation file\n",
    "    yolo_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LIRAN\\Desktop\\technion\\winter_23\\deep\\project\\Using_YOLOv8\n"
     ]
    }
   ],
   "source": [
    "%cd ../..\n",
    "# %cd Using_YOLOv8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Split to train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 3000 files [00:43, 68.56 files/s] \n"
     ]
    }
   ],
   "source": [
    "# split train and label folders to train, val, test\n",
    "!pip install split-folders\n",
    "import splitfolders\n",
    "splitfolders.ratio(save_path, output=project_path + \"/taco_dataset\", seed=1337, ratio=(.7, 0.15,0.15))\n",
    "#we use fixed seed to make sure we get the same results every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Create .yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yaml_file_name = \"taco_trash_dataset.yaml\"\n",
    "path = \"../../../../../../taco_dataset/\"\n",
    "train = \"train/images\"\n",
    "val = \"val/images\"\n",
    "test = \"test/images\"\n",
    "#get classes and their corresponding idx\n",
    "names = '\\n  '.join([str(id) + ': ' + str(cls) for cls, id in classes_ids.items()])\n",
    "names = '  ' + names\n",
    "\n",
    "txt = \"# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\\n\" \\\n",
    "      \"path: {}  # dataset root dir\\n\" \\\n",
    "      \"\\n\" \\\n",
    "      \"train: {}  # train images (relative to 'path') 128 images\\n\" \\\n",
    "      \"val: {}  # val images (relative to 'path') 128 images\\n\" \\\n",
    "      \"test: {} # test images (optional)\\n\" \\\n",
    "      \"names:\\n\" \\\n",
    "      \"{}\".format(path, train, val, test, names)\n",
    "\n",
    "with open(project_path+ \"/\" + yaml_file_name, 'w') as y_file:\n",
    "    y_file.write(txt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Update: Before and After Submission\n",
    "\n",
    "During the course, we were given a limited amount of time to work on the task, which prevented us from finding the best parameters. After submitting the assignment, I independently continued the project to further improve it through fine-tuning.\n",
    "\n",
    "Hence, I will divide the progress into two parts: the version that was submitted and the version after submission."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Submission\n",
    "\n",
    "Without parameters finetuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Using Server\n",
    "The code commands are from the ultralytics quickstart.\n",
    "https://docs.ultralytics.com/quickstart/#install\n",
    "\n",
    "#### With pretrained weights\n",
    "We tried to use pretrained weights of YOLOv8 nano version. Here's the command we use for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the following command to train the model on the server\n",
    "# replace <project_location> with your project directory located on the server\n",
    "\n",
    "#!srun -c 2 --gres=gpu:1 yolo train model=yolov8n.pt imgsz=640 data=<project_location>/dataset.yaml epochs=3 weights=yolov8n.pt cache=True\n",
    "\n",
    "#for example we used:\n",
    "# srun -c 2 --gres=gpu:1 yolo train model=yolov8n.pt imgsz=640 data=/home/liranc6/Using_YOLOv8/dataset.yaml epochs=3 weights=yolov8n.pt cache=True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Without pretrained weights\n",
    "\n",
    "We also tried to train the model from scratch, because the nano model was trained on COCO images which have different classes than TACO. Also, the number of classes we use in our dataset is much smaller that in COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Run the following command to train the model on the server\n",
    "# replace <project_location> with your project directory located on the server\n",
    "\n",
    "#!srun -c 2 --gres=gpu:1 yolo task=detect mode=yolov8.pt imgsz=640 data=<project_location>/taco_trash_dataset.yaml batch=1 epochs=3000 cache=True save_period=100\n",
    "\n",
    "#for example we used:\n",
    "#!srun -c 2 --gres=gpu:1 yolo task=detect mode=yolov8.pt imgsz=640 data=/home/liranc6/Using_YOLOv8/taco_trash_dataset.yaml batch=1 epochs=3000 cache=True save_period=100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Results\n",
    "\n",
    "As we can see in the following diagram, the labels count is not equivalents between classes: the \"bio\" and \"other\" are the least represented in the dataset and the most common label is \"metals_and_plastics\".\n",
    "\n",
    "<img src=\"project/labels_count.JPEG\" width=\"576\" height=\"324\"/>\n",
    "\n",
    "After training the model, we got the confusion matrix below:\n",
    "`\n",
    "<img src=\"project/confusion_matrix.png\" width=\"960\" height=\"540\"/>\n",
    "\n",
    "As expected, the most common label has also the best prediction accuracy.\n",
    "\n",
    "<img src=\"project/PR_curve.png\" width=\"480\" height=\"270\"/>\n",
    "\n",
    "We can see that the training loss decreases over time.\n",
    "\n",
    "<img src=\"project/results.png\" width=\"960\" height=\"270\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tests\n",
    "\n",
    "<img src=\"tests/batch_1_000004.jpg\" width=\"768\" height=\"432\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"tests/batch_2_000030.JPG\" width=\"768\" height=\"432\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"tests/batch_1_000058.jpg\" width=\"768\" height=\"432\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Submission\n",
    "\n",
    "With Parameters Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Server\n",
    "The code commands are from the ultralytics quickstart.\n",
    "https://docs.ultralytics.com/quickstart/#install\n",
    "\n",
    "#### Finding Most Promessing Hyperparameters\n",
    "I aimed to fine-tune hyperparameters for the dataset. Due to resource limitations, I conducted brief training epochs to identify promising settings. It's important to acknowledge that more epochs might have unveiled better parameter choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Expirements\n",
    "With my available resources, I chose to train for 5 epochs with each parameter combination.\n",
    "\n",
    "optimizers = ['SGD', 'Adam', 'AdamW']\n",
    "\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "\n",
    "momentums = [0.1, 0.5, 0.9]\n",
    "\n",
    "I've created a file with these lines and executed it on the server.\n",
    "\n",
    "The file is called \"expirements.py\" and is attached to the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Set up hyper-parameters\n",
    "optimizers = ['SGD', 'Adam', 'AdamW']\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "momentums = [0.1, 0.5, 0.9]\n",
    "data = \"/home/liranc6/Using_YOLOv8/taco_trash_dataset.yaml\"\n",
    "prefix_command = f\"yolo task=detect mode=train model=yolov8s.pt imgsz=640 data={data} epochs=10 workers=2\"\n",
    "suffix_command = \"2>&1 | tee \"\n",
    "\n",
    "# Run each combination for 10 epochs and collect results\n",
    "for optimizer in optimizers:\n",
    "    for learning_rate in learning_rates:\n",
    "        for momentum in momentums:\n",
    "            # Run the command to train the model with the current hyperparameters\n",
    "            name = f\"optimizer={str(optimizer)}_learning_rate={str(learning_rate)}_momentum={str(momentum)}\"  # result folder name\n",
    "            output_file = name.replace('.', '_').replace('=', '_') + \".txt\"\n",
    "            command = \"srun -c 2 --gres=gpu:1 {} optimizer={} lrf={} momentum={} name={} {} {}\" \\\n",
    "                      \"\".format(prefix_command, str(optimizer), str(learning_rate), str(momentum), name, suffix_command, output_file)\n",
    "            print(\"running now: \\n{}\".format(command))  # follow progress\n",
    "            ###run\n",
    "            output = subprocess.check_output(command, shell=True)\n",
    "            print(output.decode())\n",
    "            print(\"finished running command\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evluating results\n",
    "\n",
    "I've created a file with these lines and executed it on the server.\n",
    "\n",
    "The file is called \"Evaluate_results.py\" and is attached to the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "optimizers = ['SGD', 'Adam', 'AdamW']\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "momentums = [0.1, 0.5, 0.9]\n",
    "results_base_path = os.path.join(\"/home/liranc6/Using_YOLOv8/runs/detect/\")\n",
    "dest_base_path = \"/home/liranc6/Using_YOLOv8/results\"\n",
    "\n",
    "\"\"\"\n",
    "reads the results from path and returns a dict of dataframes when key=optimizer, val=df\n",
    "\"\"\"\n",
    "def create_results_data_frames():\n",
    "    df_dict = {}\n",
    "    # loop through the optimizers\n",
    "    for optimizer in optimizers:\n",
    "        # create an empty data frame\n",
    "        optimizer_df = pd.DataFrame(columns=momentums, index=learning_rates)\n",
    "\n",
    "        optimizer_df = optimizer_df.fillna(0)\n",
    "\n",
    "        for learning_rate in learning_rates:\n",
    "            mAP_score = 0\n",
    "            for momentum in momentums:\n",
    "                # name path\n",
    "                folder_name = f\"optimizer={str(optimizer)}_learning_rate={str(learning_rate)}_momentum={str(momentum)}\"\n",
    "\n",
    "                full_folder_name = os.path.join(results_base_path, folder_name)\n",
    "                        \n",
    "                #get mAP50-95 results\n",
    "                result_csv_path = os.path.join(full_folder_name, 'results.csv')\n",
    "                if os.path.exists(result_csv_path):\n",
    "                    df = pd.read_csv(result_csv_path)\n",
    "                    mAP_score = df['    metrics/mAP50-95(B)'].iloc[-1]\n",
    "                    optimizer_df.loc[learning_rate, momentum] = mAP_score\n",
    "\n",
    "        # add the data frame to the dictionary with the optimizer name as the key\n",
    "        df_dict[optimizer] = optimizer_df\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "def print_df(df_dict):\n",
    "    for optimizer, results in df_dict.items():\n",
    "        print(optimizer + \":\")\n",
    "        print(results)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "def create_results_csv(df_dict):\n",
    "    with open(\"results.csv\", \"w\") as f:\n",
    "        for op in optimizers:\n",
    "            f.write(str(op) + \":\\n\")\n",
    "            df_dict[op].to_csv(f, mode='a')\n",
    "            # write two new lines after each DataFrame\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "def find_best_params(df_dict):\n",
    "    best_params = \"\"\n",
    "    max = -1\n",
    "    for op in optimizers:\n",
    "        df = df_dict[op]\n",
    "        # get the row and column labels of the maximum value\n",
    "        max_row_label, max_col_label = df.stack().idxmax()\n",
    "        # get the value of the maximum cell\n",
    "        max_val = df.loc[max_row_label, max_col_label]\n",
    "        if max_val>max:\n",
    "            best_params = \"best params are: \" + str(op) + \" learning_rates=\" + str(max_row_label) + \" momentum=\" + str(max_col_label)\\\n",
    "                        + \"max_val_after_5_ephocs=\" + str(max_val)\n",
    "            \n",
    "    return best_params\n",
    "\n",
    "def print_best_params(best_params):\n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(best_params)\n",
    "    print(best_params)\n",
    "\n",
    "\n",
    "def copy_results_csv_to_new_folder(dest_base_path):\n",
    "    # Create the directory\n",
    "    os.makedirs(dest_base_path, exist_ok=True)\n",
    "    # loop through the optimizers\n",
    "    for optimizer in optimizers:\n",
    "        for learning_rate in learning_rates:\n",
    "            for momentum in momentums:\n",
    "                #rename folder\n",
    "                folder_name = f\"optimizer={str(optimizer)}_learning_rate={str(learning_rate)}_momentum={str(momentum)}\"\n",
    "                new_name = os.path.join(results_base_path, folder_name)\n",
    "\n",
    "                #get mAP50-95 results\n",
    "                result_csv_path = os.path.join(new_name, 'results.csv')\n",
    "                if os.path.exists(result_csv_path):\n",
    "                    # Set the source and destination paths\n",
    "                    src_path = result_csv_path\n",
    "\n",
    "                    dest_full_path = os.path.join(dest_base_path, folder_name+'.csv')\n",
    "\n",
    "                    # Copy the file to the destination folder\n",
    "                    shutil.copy(src_path, dest_full_path)\n",
    "                else:\n",
    "                    print(\"no folder found\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_dict = create_results_data_frames()\n",
    "\n",
    "    create_results_csv(df_dict)\n",
    "\n",
    "    copy_results_csv_to_new_folder(dest_base_path)\n",
    "\n",
    "    print_df(df_dict)\n",
    "\n",
    "    create_results_csv(df_dict)\n",
    "\n",
    "    best_params = find_best_params(df_dict)\n",
    "\n",
    "    print_best_params(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "\n",
    "**columns are momentums and rows are learning_rates\n",
    "\n",
    "<img src=\"after_submition\\choosing_params.png\" width=\"960\" height=\"540\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pretrained weights and most promessing hyperparameters\n",
    "\n",
    "The expirements shows that the most promessing parameters are using:\n",
    "\n",
    "optimizers = AdamW\n",
    "\n",
    "learning_rates = 0.01\n",
    "\n",
    "momentums = 0.9\n",
    "\n",
    "I trained the model again with those parameters for 200 ephocs.\n",
    "The resoults are as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the following command to train the model on the server\n",
    "# replace <project_location> with your project directory located on the server\n",
    "\n",
    "#!srun -c 2 --gres=gpu:1 yolo task=detect mode=yolov8.pt imgsz=640 data=<project_location>/taco_trash_dataset.yaml batch=1 epochs=3000 cache=True save_period=100\n",
    "\n",
    "#for example we used:\n",
    "#!srun -c 2 --gres=gpu:1 yolo task=detect mode=yolov8.pt imgsz=640 data=/home/liranc6/Using_YOLOv8/taco_trash_dataset.yaml batch=1 epochs=3000 cache=True save_period=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Our exploration of new hyperparameters has yielded diverse results. As you navigate through these outcomes, please don't hesitate to delve deeper and scrutinize the intricacies at your own pace.\n",
    "\n",
    "However, one persistent observation remains unchanged from our previous analysis—the class distribution is still imbalanced. \"Bio\" and \"Other\" classes are underrepresented, while \"Metals and Plastics\" continues to dominate.\n",
    "\n",
    "Here's a quick look at the confusion matrix:\n",
    "\n",
    "<img src=\"after_submition/confusion_matrix.png\" width=\"960\" height=\"540\"/> \n",
    "\n",
    "\n",
    "PR_curve:\n",
    "\n",
    "\n",
    "<img src=\"after_submition/PR_curve.png\" width=\"480\" height=\"270\"/>\n",
    "\n",
    "\n",
    "Results:\n",
    "\n",
    "<img src=\"after_submition/results.png\" width=\"960\" height=\"270\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I've provided the weights obtained after training for your convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You for Your Interest\n",
    "\n",
    "I appreciate your time and attention in reading this notebook file. \n",
    "Your thoughts and comments are always welcome.\n",
    "Thank you for taking the time to explore our work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
